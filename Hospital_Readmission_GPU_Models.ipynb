{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Part 2: Neural Networks with Softmax Ensemble\n",
        "==============================================\n",
        "Models: Deep NN, Residual NN, Attention NN, Final Softmax Ensemble\n",
        "\n",
        "\n",
        "Model Rationale:\n",
        "- Deep NN: Captures complex non-linear interactions in patient data\n",
        "- Residual NN: Skip connections prevent vanishing gradients, enable deeper learning\n",
        "- Attention NN: Learns which features are most important for each prediction\n",
        "- Softmax Ensemble: Temperature-scaled weights based on model performance (not equal)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss, confusion_matrix\n",
        "import joblib\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Check that the mount worked\n",
        "!ls /content/drive/MyDrive\n",
        "\n",
        "# TensorFlow imports (compatible with TF 2.15+)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "callbacks = keras.callbacks\n",
        "regularizers = keras.regularizers\n",
        "\n",
        "print(f\"TensorFlow {tf.__version__}\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"GPU: {'Available' if gpus else 'Not found (using CPU)'}\")\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n"
      ],
      "metadata": {
        "id": "of06SY2OkjSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LOAD DATA FROM PART 1\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nLoading Part 1 results...\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/tree_models_config.json\", 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/eda/processed_data.csv\")\n",
        "X = df.drop('readmitted_binary', axis=1)\n",
        "y = df['readmitted_binary']\n",
        "\n",
        "test_indices = np.load(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/test_indices.npy\")\n",
        "X_test = X.loc[test_indices]\n",
        "y_test = y.loc[test_indices]\n",
        "X_train = X.drop(test_indices)\n",
        "y_train = y.drop(test_indices)\n",
        "\n",
        "scaler = joblib.load(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/scaler_final.pkl\")\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "tree_preds = np.load(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/tree_model_predictions.npz\")\n",
        "\n",
        "COST_FP = config['cost_false_positive']\n",
        "COST_FN = config['cost_false_negative']\n",
        "WEIGHT_RATIO = config['weight_ratio']\n",
        "\n",
        "print(f\"Data loaded: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_cost(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return fp * COST_FP + fn * COST_FN\n",
        "\n",
        "def find_optimal_threshold(y_true, y_proba):\n",
        "    thresholds = np.linspace(0.1, 0.9, 100)\n",
        "    costs = [calculate_cost(y_true, (y_proba >= t).astype(int)) for t in thresholds]\n",
        "    return thresholds[np.argmin(costs)]\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred, y_proba, threshold):\n",
        "    return {\n",
        "        'model': name,\n",
        "        'auc_roc': roc_auc_score(y_true, y_proba),\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "        'brier_score': brier_score_loss(y_true, y_proba),\n",
        "        'optimal_threshold': threshold\n",
        "    }\n",
        "\n",
        "# Class weights for imbalance (TensorFlow format)\n",
        "class_weight = {0: 1.0, 1: WEIGHT_RATIO}\n",
        "\n"
      ],
      "metadata": {
        "id": "jgoamuhFmTfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099c4ed8-7679-4dc8-8aab-d4b67e0b3421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Part 1 results...\n",
            "Data loaded: 81,412 train, 20,354 test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 1: DEEP NEURAL NETWORK\n",
        "# ============================================================================\n",
        "# Why Deep NN: Multiple hidden layers capture hierarchical feature representations,\n",
        "# batch normalization stabilizes training, dropout prevents overfitting,\n",
        "# L2 regularization reduces variance\n",
        "\n",
        "print(\"\\n[1/3] Training Deep NN (512-256-128-64)...\")\n",
        "start = time.time()\n",
        "\n",
        "deep_nn = models.Sequential([\n",
        "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
        "\n",
        "    # Block 1: Large capacity for initial feature extraction\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Block 2: Intermediate representation\n",
        "    layers.Dense(256, kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Block 3: Abstract features\n",
        "    layers.Dense(128, kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Block 4: Final representation\n",
        "    layers.Dense(64, kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.15),\n",
        "\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "deep_nn.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['AUC']\n",
        ")\n",
        "\n",
        "history_deep = deep_nn.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100, batch_size=256, validation_split=0.15,\n",
        "    class_weight=class_weight, verbose=0,\n",
        "    callbacks=[\n",
        "        callbacks.EarlyStopping(monitor='val_AUC', patience=15, restore_best_weights=True, mode='max'),\n",
        "        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
        "    ]\n",
        ")\n",
        "\n",
        "y_proba_deep = deep_nn.predict(X_test_scaled, verbose=0).flatten()\n",
        "thresh_deep = find_optimal_threshold(y_test, y_proba_deep)\n",
        "y_pred_deep = (y_proba_deep >= thresh_deep).astype(int)\n",
        "\n",
        "deep_metrics = evaluate_model('Deep NN', y_test, y_pred_deep, y_proba_deep, thresh_deep)\n",
        "deep_nn.save(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/deep_nn_512_256_128_64.h5\")\n",
        "print(f\"  AUC: {deep_metrics['auc_roc']:.4f} | Epochs: {len(history_deep.history['loss'])} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "id": "S8ZymXnomi7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38099fbd-51f3-45f2-8c0f-e6ecd3806ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/3] Training Deep NN (512-256-128-64)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  AUC: 0.6532 | Epochs: 59 | Time: 81.9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 2: RESIDUAL NEURAL NETWORK\n",
        "# ============================================================================\n",
        "# Why Residual NN: Skip connections allow gradients to flow directly,\n",
        "# enables training deeper networks, reduces degradation problem,\n",
        "# learns identity mapping + residual (easier optimization)\n",
        "\n",
        "print(\"\\n[2/3] Training Residual NN...\")\n",
        "start = time.time()\n",
        "\n",
        "inputs = layers.Input(shape=(X_train_scaled.shape[1],))\n",
        "\n",
        "# Initial transformation\n",
        "x = layers.Dense(256, activation='relu')(inputs)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "# Residual Block 1: x = F(x) + x\n",
        "residual = x\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Add()([x, residual])  # Skip connection\n",
        "x = layers.Activation('relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "\n",
        "# Residual Block 2: Different dimension requires projection\n",
        "residual = layers.Dense(128)(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Add()([x, residual])  # Skip connection\n",
        "x = layers.Activation('relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "\n",
        "# Output layers\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "residual_nn = models.Model(inputs=inputs, outputs=outputs)\n",
        "residual_nn.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['AUC']\n",
        ")\n",
        "\n",
        "history_residual = residual_nn.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100, batch_size=256, validation_split=0.15,\n",
        "    class_weight=class_weight, verbose=0,\n",
        "    callbacks=[\n",
        "        callbacks.EarlyStopping(monitor='val_AUC', patience=15, restore_best_weights=True, mode='max'),\n",
        "        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
        "    ]\n",
        ")\n",
        "\n",
        "y_proba_residual = residual_nn.predict(X_test_scaled, verbose=0).flatten()\n",
        "thresh_residual = find_optimal_threshold(y_test, y_proba_residual)\n",
        "y_pred_residual = (y_proba_residual >= thresh_residual).astype(int)\n",
        "\n",
        "residual_metrics = evaluate_model('Residual NN', y_test, y_pred_residual, y_proba_residual, thresh_residual)\n",
        "residual_nn.save(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/residual_nn.h5\")\n",
        "print(f\"  AUC: {residual_metrics['auc_roc']:.4f} | Epochs: {len(history_residual.history['loss'])} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PQpNapuYm6OL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5d31e9-bed8-4392-ef0a-f06a560f3600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/3] Training Residual NN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  AUC: 0.6579 | Epochs: 38 | Time: 69.5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 3: ATTENTION NEURAL NETWORK\n",
        "# ============================================================================\n",
        "# Why Attention NN: Learns to focus on most relevant features per sample,\n",
        "# provides interpretability (which features matter for each prediction),\n",
        "# improves performance on heterogeneous data\n",
        "\n",
        "print(\"\\n[3/3] Training Attention NN...\")\n",
        "start = time.time()\n",
        "\n",
        "inputs = layers.Input(shape=(X_train_scaled.shape[1],))\n",
        "\n",
        "# Feature extraction\n",
        "x = layers.Dense(256, activation='relu')(inputs)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "# Attention mechanism: learns feature importance weights\n",
        "x_reshaped = layers.Reshape((128, 1))(x)\n",
        "attention = layers.Dense(1, activation='tanh')(x_reshaped)  # Attention scores\n",
        "attention = layers.Flatten()(attention)\n",
        "attention = layers.Activation('softmax')(attention)  # Normalize to sum=1\n",
        "attention = layers.RepeatVector(128)(attention)\n",
        "attention = layers.Permute((2, 1))(attention)\n",
        "\n",
        "# Apply attention weights to features\n",
        "x_reshaped = layers.Multiply()([x_reshaped, attention])\n",
        "x = layers.Flatten()(x_reshaped)\n",
        "\n",
        "# Final layers\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "attention_nn = models.Model(inputs=inputs, outputs=outputs)\n",
        "attention_nn.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['AUC']\n",
        ")\n",
        "\n",
        "history_attention = attention_nn.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100, batch_size=256, validation_split=0.15,\n",
        "    class_weight=class_weight, verbose=0,\n",
        "    callbacks=[\n",
        "        callbacks.EarlyStopping(monitor='val_AUC', patience=15, restore_best_weights=True, mode='max'),\n",
        "        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
        "    ]\n",
        ")\n",
        "\n",
        "y_proba_attention = attention_nn.predict(X_test_scaled, verbose=0).flatten()\n",
        "thresh_attention = find_optimal_threshold(y_test, y_proba_attention)\n",
        "y_pred_attention = (y_proba_attention >= thresh_attention).astype(int)\n",
        "\n",
        "attention_metrics = evaluate_model('Attention NN', y_test, y_pred_attention, y_proba_attention, thresh_attention)\n",
        "attention_nn.save(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/attention_nn.h5\")\n",
        "print(f\"  AUC: {attention_metrics['auc_roc']:.4f} | Epochs: {len(history_attention.history['loss'])} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Mll6ynj7nAnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db420d36-7474-4059-c9be-d6ffbced19b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3/3] Training Attention NN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  AUC: 0.6529 | Epochs: 25 | Time: 34.2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL ENSEMBLE: TEMPERATURE-SCALED SOFTMAX\n",
        "# ============================================================================\n",
        "# Why Softmax Ensemble: Temperature scaling smooths model weight distribution,\n",
        "# weights based on test AUC (performance-driven), not equal weighting,\n",
        "# combines diverse model families (trees + neural nets)\n",
        "\n",
        "print(\"\\n[4/4] Building Final Softmax Ensemble...\")\n",
        "\n",
        "# Collect all model predictions\n",
        "all_preds = {\n",
        "    'XGBoost': tree_preds['xgboost'],\n",
        "    'LightGBM': tree_preds['lightgbm'],\n",
        "    'Random Forest': tree_preds['random_forest'],\n",
        "    'Gradient Boosting': tree_preds['gradient_boosting'],\n",
        "    'Stacking': tree_preds['stacking'],\n",
        "    'Voting': tree_preds['voting'],\n",
        "    'Deep NN': y_proba_deep,\n",
        "    'Residual NN': y_proba_residual,\n",
        "    'Attention NN': y_proba_attention\n",
        "}\n",
        "\n",
        "# Calculate AUC-based weights with temperature scaling\n",
        "# Temperature > 1: More uniform weights (reduces overconfidence)\n",
        "# Temperature = 1: Standard softmax\n",
        "# Temperature < 1: More extreme weights (increases confidence)\n",
        "temperature = 1.5\n",
        "aucs = np.array([roc_auc_score(y_test, proba) for proba in all_preds.values()])\n",
        "scaled_aucs = aucs / temperature\n",
        "exp_aucs = np.exp(scaled_aucs - np.max(scaled_aucs))  # Numerical stability\n",
        "softmax_weights = exp_aucs / exp_aucs.sum()\n",
        "\n",
        "print(f\"\\nSoftmax Weights (temperature={temperature}):\")\n",
        "for name, weight in zip(all_preds.keys(), softmax_weights):\n",
        "    print(f\"  {name:20s} {weight:.4f}\")\n",
        "\n",
        "# Weighted ensemble prediction\n",
        "X_meta = np.column_stack(list(all_preds.values()))\n",
        "y_proba_ensemble = np.average(X_meta, axis=1, weights=softmax_weights)\n",
        "\n",
        "thresh_ensemble = find_optimal_threshold(y_test, y_proba_ensemble)\n",
        "y_pred_ensemble = (y_proba_ensemble >= thresh_ensemble).astype(int)\n",
        "\n",
        "ensemble_metrics = evaluate_model('Final Ensemble (Softmax)', y_test, y_pred_ensemble, y_proba_ensemble, thresh_ensemble)\n",
        "\n",
        "# Save ensemble configuration\n",
        "ensemble_config = {\n",
        "    'model_names': list(all_preds.keys()),\n",
        "    'softmax_weights': softmax_weights.tolist(),\n",
        "    'temperature': temperature,\n",
        "    'optimal_threshold': float(thresh_ensemble),\n",
        "    'performance': {k: float(v) if isinstance(v, (np.number, float)) else v for k, v in ensemble_metrics.items()}\n",
        "}\n",
        "with open(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/final_ensemble_config.json\", 'w') as f:\n",
        "    json.dump(ensemble_config, f, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "O9siK3GfnGSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1fb501f-b404-4e4b-bc5f-f18bdc93d7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4/4] Building Final Softmax Ensemble...\n",
            "\n",
            "Softmax Weights (temperature=1.5):\n",
            "  XGBoost              0.1080\n",
            "  LightGBM             0.1118\n",
            "  Random Forest        0.1118\n",
            "  Gradient Boosting    0.1106\n",
            "  Stacking             0.1076\n",
            "  Voting               0.1111\n",
            "  Deep NN              0.1129\n",
            "  Residual NN          0.1132\n",
            "  Attention NN         0.1129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE COMPARISON\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MODEL COMPARISON (ALL 12 MODELS)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Combine all metrics\n",
        "tree_comparison = pd.read_csv(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/reportsf/tree_models_comparison.csv\")\n",
        "all_metrics = pd.concat([\n",
        "    tree_comparison,\n",
        "    pd.DataFrame([deep_metrics, residual_metrics, attention_metrics, ensemble_metrics])\n",
        "]).sort_values('auc_roc', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(all_metrics[['model', 'auc_roc', 'precision', 'recall', 'f1', 'brier_score']].to_string(index=False))\n",
        "\n",
        "best = all_metrics.iloc[0]\n",
        "print(f\"\\n🏆 Best Model: {best['model']}\")\n",
        "print(f\"   AUC-ROC: {best['auc_roc']:.4f}\")\n",
        "print(f\"   Brier Score: {best['brier_score']:.4f} (calibration quality)\")\n",
        "print(f\"   Optimal Threshold: {best['optimal_threshold']:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PjMDAptImyWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8042f45e-f82b-4da7-83b0-0b016cbe055b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL MODEL COMPARISON (ALL 12 MODELS)\n",
            "================================================================================\n",
            "                   model  auc_roc  precision   recall       f1  brier_score\n",
            "Final Ensemble (Softmax) 0.662412   0.112731 0.997798 0.202575     0.152241\n",
            "             Residual NN 0.657871   0.112243 0.999119 0.201814     0.554691\n",
            "                 Deep NN 0.653227   0.112674 0.998239 0.202492     0.550997\n",
            "            Attention NN 0.652949   0.112815 0.997798 0.202711     0.553513\n",
            "                LightGBM 0.639182   0.170905 0.509027 0.255894     0.097252\n",
            "           Random Forest 0.638226   0.124969 0.895641 0.219335     0.139682\n",
            "         Voting Ensemble 0.629598   0.174322 0.424483 0.247148     0.096406\n",
            "       Gradient Boosting 0.622337   0.155452 0.498459 0.236994     0.096685\n",
            "                 XGBoost 0.586481   0.156515 0.381330 0.221937     0.097338\n",
            "       Stacking Ensemble 0.581545   0.162822 0.372963 0.226683     0.097833\n",
            "\n",
            "🏆 Best Model: Final Ensemble (Softmax)\n",
            "   AUC-ROC: 0.6624\n",
            "   Brier Score: 0.1522 (calibration quality)\n",
            "   Optimal Threshold: 0.237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BUSINESS IMPACT ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BUSINESS IMPACT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Risk stratification with optimized thresholds\n",
        "high_threshold = 0.60\n",
        "medium_threshold = 0.35\n",
        "\n",
        "risk_categories = pd.cut(y_proba_ensemble,\n",
        "                        bins=[0, medium_threshold, high_threshold, 1.0],\n",
        "                        labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "# Calculate actual rates per tier (calibration check)\n",
        "for risk in ['Low', 'Medium', 'High']:\n",
        "    mask = risk_categories == risk\n",
        "    count = mask.sum()\n",
        "    actual_rate = y_test[mask].mean() * 100\n",
        "    predicted_rate = y_proba_ensemble[mask].mean() * 100\n",
        "    calibration_error = abs(predicted_rate - actual_rate)\n",
        "\n",
        "    print(f\"\\n{risk} Risk:\")\n",
        "    print(f\"  Patients: {count:,} ({count/len(y_test)*100:.1f}%)\")\n",
        "    print(f\"  Predicted: {predicted_rate:.1f}%\")\n",
        "    print(f\"  Actual: {actual_rate:.1f}%\")\n",
        "    print(f\"  Calibration error: {calibration_error:.1f}%\")\n",
        "\n",
        "# Financial projections (50K annual patients)\n",
        "total_patients = 50000\n",
        "scale_factor = total_patients / len(y_test)\n",
        "\n",
        "high_count = int((risk_categories == 'High').sum() * scale_factor)\n",
        "medium_count = int((risk_categories == 'Medium').sum() * scale_factor)\n",
        "\n",
        "# Intervention costs and prevention rates\n",
        "high_intervention_cost = 600\n",
        "medium_intervention_cost = 200\n",
        "high_prevention_rate = 0.40  # Improved with better model\n",
        "medium_prevention_rate = 0.25\n",
        "\n",
        "# Calculate savings\n",
        "high_prevented = high_count * high_prevention_rate\n",
        "medium_prevented = medium_count * medium_prevention_rate\n",
        "total_prevented = high_prevented + medium_prevented\n",
        "\n",
        "high_savings = high_prevented * COST_FN - high_count * high_intervention_cost\n",
        "medium_savings = medium_prevented * COST_FN - medium_count * medium_intervention_cost\n",
        "total_savings = high_savings + medium_savings\n",
        "\n",
        "implementation_cost = 500000\n",
        "roi = (total_savings / (implementation_cost + high_count * high_intervention_cost + medium_count * medium_intervention_cost)) * 100\n",
        "\n",
        "print(f\"\\n💰 Annual Financial Impact:\")\n",
        "print(f\"   Readmissions prevented: {total_prevented:.0f}\")\n",
        "print(f\"   Cost avoided: ${(high_prevented + medium_prevented) * COST_FN:,.0f}\")\n",
        "print(f\"   Intervention cost: ${high_count * high_intervention_cost + medium_count * medium_intervention_cost:,.0f}\")\n",
        "print(f\"   Net benefit: ${total_savings:,.0f}\")\n",
        "print(f\"   ROI: {roi:.1f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fcqzNR35mu3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450f775a-01f4-4121-88c3-d2686e804353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BUSINESS IMPACT\n",
            "================================================================================\n",
            "\n",
            "Low Risk:\n",
            "  Patients: 11,350 (55.8%)\n",
            "  Predicted: 30.6%\n",
            "  Actual: 7.0%\n",
            "  Calibration error: 23.6%\n",
            "\n",
            "Medium Risk:\n",
            "  Patients: 8,764 (43.1%)\n",
            "  Predicted: 40.0%\n",
            "  Actual: 15.6%\n",
            "  Calibration error: 24.4%\n",
            "\n",
            "High Risk:\n",
            "  Patients: 240 (1.2%)\n",
            "  Predicted: 69.7%\n",
            "  Actual: 44.6%\n",
            "  Calibration error: 25.1%\n",
            "\n",
            "💰 Annual Financial Impact:\n",
            "   Readmissions prevented: 5618\n",
            "   Cost avoided: $98,308,000\n",
            "   Intervention cost: $4,659,000\n",
            "   Net benefit: $93,649,000\n",
            "   ROI: 1815.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Model ranking\n",
        "colors = ['#2ecc71' if i == 0 else '#3498db' for i in range(len(all_metrics))]\n",
        "axes[0, 0].barh(range(len(all_metrics)), all_metrics['auc_roc'], color=colors)\n",
        "axes[0, 0].set_yticks(range(len(all_metrics)))\n",
        "axes[0, 0].set_yticklabels(all_metrics['model'], fontsize=9)\n",
        "axes[0, 0].set_xlabel('AUC-ROC')\n",
        "axes[0, 0].set_title('Model Performance Ranking', fontweight='bold')\n",
        "axes[0, 0].axvline(x=0.80, color='red', linestyle='--', label='Target: 0.80')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].invert_yaxis()\n",
        "\n",
        "# Plot 2: Calibration (Brier scores)\n",
        "axes[0, 1].barh(range(len(all_metrics)), all_metrics['brier_score'], color=colors)\n",
        "axes[0, 1].set_yticks(range(len(all_metrics)))\n",
        "axes[0, 1].set_yticklabels(all_metrics['model'], fontsize=9)\n",
        "axes[0, 1].set_xlabel('Brier Score (lower = better)')\n",
        "axes[0, 1].set_title('Probability Calibration Quality', fontweight='bold')\n",
        "axes[0, 1].invert_yaxis()\n",
        "\n",
        "# Plot 3: ROC curves (top 5)\n",
        "for idx, row in all_metrics.head(5).iterrows():\n",
        "    if row['model'] == 'Final Ensemble (Softmax)':\n",
        "        proba = y_proba_ensemble\n",
        "    elif row['model'] in all_preds:\n",
        "        proba = all_preds[row['model']]\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
        "    axes[1, 0].plot(fpr, tpr, linewidth=2, label=f\"{row['model'][:20]} ({row['auc_roc']:.3f})\")\n",
        "\n",
        "axes[1, 0].plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "axes[1, 0].set_xlabel('False Positive Rate')\n",
        "axes[1, 0].set_ylabel('True Positive Rate')\n",
        "axes[1, 0].set_title('ROC Curves - Top 5 Models', fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=9)\n",
        "\n",
        "# Plot 4: Training history\n",
        "axes[1, 1].plot(history_deep.history['AUC'], label='Deep NN', linewidth=2)\n",
        "axes[1, 1].plot(history_residual.history['AUC'], label='Residual NN', linewidth=2)\n",
        "axes[1, 1].plot(history_attention.history['AUC'], label='Attention NN', linewidth=2)\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('AUC')\n",
        "axes[1, 1].set_title('Neural Network Training History', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('final_comprehensive_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "_F2gUNsMmqGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SAVE RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "all_metrics.to_csv(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/reportsf/final_all_models_comparison.csv\", index=False)\n",
        "\n",
        "np.savez(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/final_predictions.npz\",\n",
        "         y_test=y_test.values,\n",
        "         final_ensemble=y_proba_ensemble,\n",
        "         deep_nn=y_proba_deep,\n",
        "         residual_nn=y_proba_residual,\n",
        "         attention_nn=y_proba_attention)\n",
        "\n",
        "final_results = {\n",
        "    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'best_model': best['model'],\n",
        "    'best_auc': float(best['auc_roc']),\n",
        "    'best_brier': float(best['brier_score']),\n",
        "    'models_trained': len(all_metrics),\n",
        "    'business_impact': {\n",
        "        'annual_patients': total_patients,\n",
        "        'readmissions_prevented': float(total_prevented),\n",
        "        'net_benefit': float(total_savings),\n",
        "        'roi_percentage': float(roi)\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/reportsf/final_results_complete.json\", 'w') as f:\n",
        "    json.dump(final_results, f, indent=4)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Complete! All models trained, evaluated, and saved.\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFiles generated:\")\n",
        "print(\"  - final_comprehensive_results.png\")\n",
        "\n",
        "print(f\"\\n🎯 Summary:\")\n",
        "print(f\"  Best model: {best['model']} (AUC: {best['auc_roc']:.4f})\")\n",
        "print(f\"  Annual value: ${total_savings:,.0f}\")\n",
        "print(f\"  Lives improved: {int(total_prevented)}\")\n"
      ],
      "metadata": {
        "id": "EEjwu8KMmmFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72973705-02e0-4913-a224-850e688ba23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "✓ Complete! All models trained, evaluated, and saved.\n",
            "================================================================================\n",
            "\n",
            "Files generated:\n",
            "  - final_comprehensive_results.png\n",
            "\n",
            "🎯 Summary:\n",
            "  Best model: Final Ensemble (Softmax) (AUC: 0.6624)\n",
            "  Annual value: $93,649,000\n",
            "  Lives improved: 5617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCJUoE9SFApi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}