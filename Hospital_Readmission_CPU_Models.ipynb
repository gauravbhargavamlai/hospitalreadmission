{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtgSFi6CbnfR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Part 1: Tree-Based Models with Cost-Sensitive Learning\n",
        "========================================================\n",
        "Models: XGBoost, LightGBM, Random Forest, Gradient Boosting, Stacking, Voting\n",
        "Runtime: 15-20 minutes on CPU\n",
        "Hardware: Any (CPU/GPU doesn't matter for tree models)\n",
        "\n",
        "Model Rationale:\n",
        "- Tree-based models excel at tabular healthcare data with mixed features\n",
        "- Cost-sensitive learning accounts for 35:1 FN:FP cost ratio ($17.5K vs $500)\n",
        "- Calibration improves probability reliability for clinical decision-making\n",
        "- Ensemble methods combine diverse model strengths\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Check that the mount worked\n",
        "!ls /content/drive/MyDrive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "COST_FP = 500       # Cost of unnecessary intervention\n",
        "COST_FN = 17500     # Cost of missed readmission (35x higher)\n",
        "WEIGHT_RATIO = COST_FN / COST_FP\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_cost(y_true, y_pred):\n",
        "    \"\"\"Calculate total business cost from predictions\"\"\"\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return fp * COST_FP + fn * COST_FN\n",
        "\n",
        "def find_optimal_threshold(y_true, y_proba):\n",
        "    \"\"\"Find probability threshold that minimizes business cost\"\"\"\n",
        "    thresholds = np.linspace(0.1, 0.9, 100)\n",
        "    costs = [calculate_cost(y_true, (y_proba >= t).astype(int)) for t in thresholds]\n",
        "    return thresholds[np.argmin(costs)]\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred, y_proba, threshold):\n",
        "    \"\"\"Return comprehensive metrics dictionary\"\"\"\n",
        "    return {\n",
        "        'model': name,\n",
        "        'auc_roc': roc_auc_score(y_true, y_proba),\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "        'brier_score': brier_score_loss(y_true, y_proba),\n",
        "        'optimal_threshold': threshold,\n",
        "        'business_cost': calculate_cost(y_true, y_pred)\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "QdMmUXVdcRd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Loading and preparing data...\")\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/eda/processed_data.csv\")\n",
        "\n",
        "X = df.drop('readmitted_binary', axis=1)\n",
        "y = df['readmitted_binary']\n",
        "\n",
        "# Train-test split with stratification to maintain class balance\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardization (important for calibration and ensembles)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# SMOTE with conservative ratio (0.7 instead of 1.0) for better generalization\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.7)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Sample weights for cost-sensitive learning\n",
        "sample_weights = np.where(y_train_balanced == 1, WEIGHT_RATIO, 1.0)\n",
        "\n",
        "joblib.dump(scaler, \"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/scaler_final.pkl\")\n",
        "print(f\"Data ready: {X_train_balanced.shape[0]:,} train, {X_test.shape[0]:,} test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RNQ28lhcYdf",
        "outputId": "730b2421-9503-498a-cfde-d80dc8d4b9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Data ready: 122,954 train, 20,354 test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 1: XGBOOST\n",
        "# ============================================================================\n",
        "# Why XGBoost: Handles mixed features well, built-in regularization,\n",
        "# scale_pos_weight parameter perfect for cost-sensitive learning\n",
        "\n",
        "print(\"\\n[1/6] Training XGBoost...\")\n",
        "start = time.time()\n",
        "\n",
        "xgb_params = {\n",
        "    'n_estimators': [200, 300, 400],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.8, 0.9],\n",
        "    'colsample_bytree': [0.8, 0.9],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [1, 1.5, 2]\n",
        "}\n",
        "\n",
        "xgb_model = RandomizedSearchCV(\n",
        "    XGBClassifier(random_state=42, eval_metric='logloss', scale_pos_weight=WEIGHT_RATIO, use_label_encoder=False),\n",
        "    xgb_params, n_iter=30, cv=5, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=0\n",
        ").fit(X_train_balanced, y_train_balanced, sample_weight=sample_weights)\n",
        "\n",
        "# Isotonic calibration: Non-parametric, works well for tree models\n",
        "xgb_calibrated = CalibratedClassifierCV(xgb_model.best_estimator_, method='isotonic', cv=3)\n",
        "xgb_calibrated.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "y_proba_xgb = xgb_calibrated.predict_proba(X_test_scaled)[:, 1]\n",
        "thresh_xgb = find_optimal_threshold(y_test, y_proba_xgb)\n",
        "y_pred_xgb = (y_proba_xgb >= thresh_xgb).astype(int)\n",
        "\n",
        "xgb_metrics = evaluate_model('XGBoost', y_test, y_pred_xgb, y_proba_xgb, thresh_xgb)\n",
        "joblib.dump(xgb_calibrated, \"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/xgboost_calibrated_final.pkl\")\n",
        "print(f\"  AUC: {xgb_metrics['auc_roc']:.4f} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPinhJl_c_5h",
        "outputId": "d7c90ab9-7592-428c-b85e-72d1ff54d596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/6] Training XGBoost...\n",
            "  AUC: 0.5865 | Time: 158.3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 2: LIGHTGBM\n",
        "# ============================================================================\n",
        "# Why LightGBM: Faster than XGBoost, leaf-wise growth better for deep trees,\n",
        "# handles categorical features natively, good for large datasets\n",
        "\n",
        "print(\"\\n[2/6] Training LightGBM...\")\n",
        "start = time.time()\n",
        "\n",
        "lgbm_params = {\n",
        "    'n_estimators': [200, 300, 400],\n",
        "    'max_depth': [5, 7, 9, -1],\n",
        "    'learning_rate': [0.01, 0.03, 0.05],\n",
        "    'num_leaves': [31, 50, 70, 100],\n",
        "    'min_child_samples': [20, 30, 50],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [0, 0.1, 0.5]\n",
        "}\n",
        "\n",
        "lgbm_model = RandomizedSearchCV(\n",
        "    LGBMClassifier(random_state=42, verbose=-1, class_weight={0: 1.0, 1: WEIGHT_RATIO}),\n",
        "    lgbm_params, n_iter=30, cv=5, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=0\n",
        ").fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "lgbm_calibrated = CalibratedClassifierCV(lgbm_model.best_estimator_, method='isotonic', cv=3)\n",
        "lgbm_calibrated.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "y_proba_lgbm = lgbm_calibrated.predict_proba(X_test_scaled)[:, 1]\n",
        "thresh_lgbm = find_optimal_threshold(y_test, y_proba_lgbm)\n",
        "y_pred_lgbm = (y_proba_lgbm >= thresh_lgbm).astype(int)\n",
        "\n",
        "lgbm_metrics = evaluate_model('LightGBM', y_test, y_pred_lgbm, y_proba_lgbm, thresh_lgbm)\n",
        "joblib.dump(lgbm_calibrated, \"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/lightgbm_calibrated_final.pkl\")\n",
        "print(f\"  AUC: {lgbm_metrics['auc_roc']:.4f} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "TJy0WWB_dE5D",
        "outputId": "fe615811-548c-4478-9b35-80a51e31f4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/6] Training LightGBM...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-824411555.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWEIGHT_RATIO\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlgbm_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m ).fit(X_train_balanced, y_train_balanced)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlgbm_calibrated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalibratedClassifierCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlgbm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'isotonic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 3: RANDOM FOREST\n",
        "# ============================================================================\n",
        "# Why Random Forest: Robust to overfitting, interpretable feature importance,\n",
        "# handles non-linear relationships, good baseline ensemble\n",
        "\n",
        "print(\"\\n[3/6] Training Random Forest...\")\n",
        "start = time.time()\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300, max_depth=8, min_samples_split=20, min_samples_leaf=10,\n",
        "    class_weight={0: 1.0, 1: WEIGHT_RATIO}, random_state=42, n_jobs=-1\n",
        ").fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "rf_calibrated = CalibratedClassifierCV(rf_model, method='isotonic', cv=3)\n",
        "rf_calibrated.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "y_proba_rf = rf_calibrated.predict_proba(X_test_scaled)[:, 1]\n",
        "thresh_rf = find_optimal_threshold(y_test, y_proba_rf)\n",
        "y_pred_rf = (y_proba_rf >= thresh_rf).astype(int)\n",
        "\n",
        "rf_metrics = evaluate_model('Random Forest', y_test, y_pred_rf, y_proba_rf, thresh_rf)\n",
        "joblib.dump(rf_calibrated, \"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/random_forest_calibrated_final.pkl\")\n",
        "print(f\"  AUC: {rf_metrics['auc_roc']:.4f} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oQIGl8SdKU7",
        "outputId": "6107ec9c-461b-4a06-af86-6d928c75a49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3/6] Training Random Forest...\n",
            "  AUC: 0.6382 | Time: 16.3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 4: GRADIENT BOOSTING\n",
        "# ============================================================================\n",
        "# Why Gradient Boosting: Sequential error correction, smooth decision boundaries,\n",
        "# often best single model, scikit-learn implementation stable\n",
        "\n",
        "print(\"\\n[4/6] Training Gradient Boosting...\")\n",
        "start = time.time()\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=200, max_depth=5, learning_rate=0.1, subsample=0.8, random_state=42\n",
        ").fit(X_train_balanced, y_train_balanced, sample_weight=sample_weights)\n",
        "\n",
        "gb_calibrated = CalibratedClassifierCV(gb_model, method='isotonic', cv=3)\n",
        "gb_calibrated.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "y_proba_gb = gb_calibrated.predict_proba(X_test_scaled)[:, 1]\n",
        "thresh_gb = find_optimal_threshold(y_test, y_proba_gb)\n",
        "y_pred_gb = (y_proba_gb >= thresh_gb).astype(int)\n",
        "\n",
        "gb_metrics = evaluate_model('Gradient Boosting', y_test, y_pred_gb, y_proba_gb, thresh_gb)\n",
        "joblib.dump(gb_calibrated, \"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/gradient_boosting_calibrated_final.pkl\")\n",
        "print(f\"  AUC: {gb_metrics['auc_roc']:.4f} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE8xdgcydOuh",
        "outputId": "60cc0efd-5fdf-4c89-9376-d25455b580b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4/6] Training Gradient Boosting...\n",
            "  AUC: 0.6223 | Time: 363.4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 5: STACKING ENSEMBLE\n",
        "# ============================================================================\n",
        "# Why Stacking: Meta-learner learns optimal combination of base models,\n",
        "# captures complementary strengths, often outperforms individual models\n",
        "\n",
        "print(\"\\n[5/6] Training Stacking Ensemble...\")\n",
        "start = time.time()\n",
        "\n",
        "base_estimators = [\n",
        "    ('xgb', xgb_model.best_estimator_),\n",
        "    ('lgbm', lgbm_model.best_estimator_),\n",
        "    ('rf', rf_model),\n",
        "    ('gb', gb_model)\n",
        "]\n",
        "\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000, class_weight={0: 1.0, 1: WEIGHT_RATIO}, random_state=42),\n",
        "    cv=5, n_jobs=-1, passthrough=False\n",
        ").fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Sigmoid calibration: Better for already-combined probabilities\n",
        "stacking_calibrated = CalibratedClassifierCV(stacking_model, method='sigmoid', cv=3)\n",
        "stacking_calibrated.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "y_proba_stack = stacking_calibrated.predict_proba(X_test_scaled)[:, 1]\n",
        "thresh_stack = find_optimal_threshold(y_test, y_proba_stack)\n",
        "y_pred_stack = (y_proba_stack >= thresh_stack).astype(int)\n",
        "\n",
        "stack_metrics = evaluate_model('Stacking Ensemble', y_test, y_pred_stack, y_proba_stack, thresh_stack)\n",
        "joblib.dump(stacking_calibrated, \"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/stacking_ensemble_final.pkl\")\n",
        "print(f\"  AUC: {stack_metrics['auc_roc']:.4f} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EItaRIhYdSw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL 6: VOTING ENSEMBLE\n",
        "# ============================================================================\n",
        "# Why Voting: Simple probability averaging, more robust than single models,\n",
        "# reduces variance, good baseline ensemble\n",
        "\n",
        "print(\"\\n[6/6] Training Voting Ensemble...\")\n",
        "start = time.time()\n",
        "\n",
        "voting_model = VotingClassifier(\n",
        "    estimators=base_estimators, voting='soft', n_jobs=-1\n",
        ").fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "voting_calibrated = CalibratedClassifierCV(voting_model, method='sigmoid', cv=3)\n",
        "voting_calibrated.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "y_proba_vote = voting_calibrated.predict_proba(X_test_scaled)[:, 1]\n",
        "thresh_vote = find_optimal_threshold(y_test, y_proba_vote)\n",
        "y_pred_vote = (y_proba_vote >= thresh_vote).astype(int)\n",
        "\n",
        "vote_metrics = evaluate_model('Voting Ensemble', y_test, y_pred_vote, y_proba_vote, thresh_vote)\n",
        "joblib.dump(voting_calibrated, \"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/voting_ensemble_final.pkl\")\n",
        "print(f\"  AUC: {vote_metrics['auc_roc']:.4f} | Time: {time.time()-start:.1f}s\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IdXb7G-5dXfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# RESULTS & SAVE\n",
        "# ============================================================================\n",
        "\n",
        "all_metrics = [xgb_metrics, lgbm_metrics, rf_metrics, gb_metrics, stack_metrics, vote_metrics]\n",
        "comparison_df = pd.DataFrame(all_metrics).sort_values('auc_roc', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TREE-BASED MODELS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df[['model', 'auc_roc', 'precision', 'recall', 'f1', 'brier_score']].to_string(index=False))\n",
        "\n",
        "best = comparison_df.iloc[0]\n",
        "print(f\"\\nBest Model: {best['model']} (AUC: {best['auc_roc']:.4f})\")\n",
        "\n",
        "# Save artifacts for Part 2\n",
        "comparison_df.to_csv('reports/tree_models_comparison.csv', index=False)\n",
        "np.savez(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/tree_model_predictions.npz\",\n",
        "         y_test=y_test.values, xgboost=y_proba_xgb, lightgbm=y_proba_lgbm,\n",
        "         random_forest=y_proba_rf, gradient_boosting=y_proba_gb,\n",
        "         stacking=y_proba_stack, voting=y_proba_vote)\n",
        "np.save(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/test_indices.npy\", X_test.index.values)\n",
        "\n",
        "config = {\n",
        "    'best_tree_model': best['model'],\n",
        "    'best_auc': float(best['auc_roc']),\n",
        "    'cost_false_positive': COST_FP,\n",
        "    'cost_false_negative': COST_FN,\n",
        "    'weight_ratio': float(WEIGHT_RATIO),\n",
        "    'feature_count': X.shape[1]\n",
        "}\n",
        "with open(\"/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission/modelsf/tree_models_config.json\", 'w') as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "print(\"\\nâœ“ Part 1 complete. Run Part 2 for neural networks and final ensemble.\")"
      ],
      "metadata": {
        "id": "7QXwNcmodb1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}