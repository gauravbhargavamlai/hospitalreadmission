{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQf4ElYSQzJq",
        "outputId": "a7512adf-992e-4596-8e06-e6772c877405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'1708094542 BHARG.pdf'\n",
            " 2012-05-09catalystdallas-120509090645-phpapp02.pdf\n",
            " 2985fdf99edf9d4dc42251eb1a26277d.jpg\n",
            " 2Print\n",
            " 30-day-ebook1.pdf\n",
            "'367 Avenida Manzanos.pdf'\n",
            "'3 BHK Duplex independent house.gdoc'\n",
            " 470-G1324.pdf\n",
            " 574062db9f183d363bcb1d9f03e69fc1.jpg\n",
            " 982763fcbd9dd911c0ea6e0b44618869d07b.pdf\n",
            " a470c4a117c6dad31a08568e310ad4a5.jpg\n",
            " Accenture-DevOps-brochure-new.pdf\n",
            " AI98036FU.pdf\n",
            "'AMWAY Report .pdf'\n",
            "'Attraction Course'\n",
            " AWS_certified_devops_engineer_professional_blueprint.pdf\n",
            " Backpropagation-Algorithm-An-Artificial-Neural-Network-Approach-for-Pattern-Recognition.pdf\n",
            " backup-oldlaptop\n",
            "'Become A Fan [Form].gform'\n",
            "'Become A Fan.gsheet'\n",
            " BerkeleyML\n",
            " bettersearchenginetesting-110324134324-phpapp02.pdf\n",
            " BizDocs\n",
            " Books\n",
            " Building-a-Brand-eBook-Section-1.pdf\n",
            " Business_India.gslides\n",
            "'Business Tracking'\n",
            "\"Changing Bits_ Testing Lucene's index durability after crash or power loss.pdf\"\n",
            "'Christmas Boutique at Oâ€™Connor Hospital.gdoc'\n",
            "'Colab Notebooks'\n",
            " conet.gsheet\n",
            " Continuous_Testing_for_DevOps_CI_CD.pdf\n",
            "'Copy of Maturity Benchmarks Survey Sheet.gsheet'\n",
            "'Copy of Recruiting-Tracker for Ben & Mayra.gsheet'\n",
            "'Copy of Sales Lead Template.gsheet'\n",
            "'Copy of Sales Page Template.gsite'\n",
            "'Copy of Team Report Template MASTER.gsheet'\n",
            "'Copy of Weekly Team Breakfast, Email Notifier (Rev. 5-2012), PUBLIC.gsheet'\n",
            "'Counselling Dec 3.pdf'\n",
            " creating_your_personal_life_plan.pdf\n",
            "'CWW Group CAL BAY Marble INC Professional Services Agreement .docx'\n",
            "'CWW Group Files â€” Attachments from Classic Sites'\n",
            "'CWW Group Files â€” Converted from Classic Sites.gsite'\n",
            " d110224ac01f10e7472087c2dd7eb14a.jpg\n",
            " d1-4-00pm-rm2-machinelearningandtesting-paulmerril-170515121709.pdf\n",
            " d3c66fa60a910f9d629e8ddbc671c208.jpg\n",
            " d855b635ddbf5d3d2dcc3e5713f3f429.jpg\n",
            "'Deep Learning notes.pdf'\n",
            "'Email Threading'\n",
            " espring_price_comparison_mar2015.pdf\n",
            " EspringWaterPurifier.pdf\n",
            " expo1.pdf\n",
            " expo2.pdf\n",
            " expo3.pdf\n",
            " f5536d1262236b7af8ed7d3a3211abeb.jpg\n",
            " fellner-sma2015.pdf\n",
            " Finances\n",
            "'Friends and Family Coupon _ Walgreens.pdf'\n",
            "'FUNNEL INTENSIVE SPREADSHEETS.gsheet'\n",
            "'Gmail - Event Confirmation_ SAN FRANCISCO, CA - Crowne Plaza, 1177 Airport Blvd.pdf'\n",
            "'Gmail - Shop Zales Gaurav and Enjoy $50 Off!.pdf'\n",
            " GoDiamond2015-Photos\n",
            "'harpreet pending.gsheet'\n",
            " Helmstetter__Shad_-_What_to_Say_When_You_Talk_to_Your_Self.pdf\n",
            "'Holiday Shopping Fair Vendor App 2007 - SF[1].gdoc'\n",
            " IMG-20151028-WA0002.jpg\n",
            " infer_fosdem2017.pdf\n",
            "'Information Security'\n",
            "'Inventory 011208.gsheet'\n",
            " inventory_011308.gsheet\n",
            "'LEAF - PCs.pdf'\n",
            "'LEAF - PLCs.pdf'\n",
            " LIVE_RIBBON_REDEMPTIONCODES_091012.gsheet\n",
            " LOS_addresses.gdoc\n",
            " machinelearninginsoftwaretesting-150428074811-conversion-gate02.pdf\n",
            " machine-learning-qa.pdf\n",
            " Marketing\n",
            "'Master DB.gsheet'\n",
            " ml-class-master.zip\n",
            " Network-is-Net-Worth-Part-1.pdf\n",
            "'Neural network'\n",
            "'New Folder'\n",
            " nutriliteproductscomparison.gsheet\n",
            " nutrition\n",
            " orderNov2012.xlsx\n",
            " partnerstores.pdf\n",
            "'Performance Test Workload Modeling.pdf'\n",
            " Personal\n",
            " PersonalAnalysisAppOLD.pdf\n",
            " PhaseA-Training\n",
            "'Picnic Sign up.gdoc'\n",
            " Pictures\n",
            "'PNA Sheet Jaideep.gsheet'\n",
            "'PNA Sheet Prabuu.gsheet'\n",
            "'PNA Sheet Template.gsheet'\n",
            " Porter_Gale_Social_Media_Case_Studies.pdf\n",
            " Position_Paper_Bayesian_Reasoning_for_Software_Testing_-_p349.pdf\n",
            " Presentations\n",
            " print\n",
            " Print.pdf\n",
            " Profile_sample_resume_v1.0.pdf\n",
            "'prsp-amw-frm-v-en--IBORegistrationForm (1).pdf'\n",
            " prsp-amw-frm-v-en--IBORegistrationForm.pdf\n",
            " QADocs\n",
            "'QA Strategy'\n",
            " rajeswar_details.png\n",
            "'Records Manager'\n",
            "'Reddy Recognitions'\n",
            " Resume\n",
            "'Ribbon Price List Updated.xlsx'\n",
            "'Sales BD candidates.gsheet'\n",
            " ScaryCloseFiveKindsofManipulators.pdf\n",
            " ScrewItLetsDoIt.pdf\n",
            "'Searching and Indexing With Apache Lucene - DZone Database.pdf'\n",
            " seke11.pdf\n",
            "'Sequoia YMCA Holiday Craft Fair. 3rd.gdoc'\n",
            " SevenHabits.pdf\n",
            " Shaurya\n",
            "'Shaurya Carden'\n",
            " slides.pdf\n",
            " Softwaretesting-amachinelearningexperiment.pdf\n",
            "'star-points-tracking-sheet-jan (1).docx'\n",
            " star-points-tracking-sheet-jan.docx\n",
            " Storyline-DonaldMiller-FirstChapter.pdf\n",
            " storyline-productivity-schedule.pdf\n",
            " Takeout\n",
            "'Takeout (1)'\n",
            "'Takeout (2)'\n",
            "'Takeout (3)'\n",
            "'Test Cases of Google Search.pdf'\n",
            "'Test-Give The Best_ How to test search functionality.pdf'\n",
            "'Testing for performance, part 1_ Assess the problem space.pdf'\n",
            "'Testing for performance, part 2_ Build out the test assets.pdf'\n",
            "'Testing Services_Performance Modeling & Workload Modeling - Are they one and the same_.pdf'\n",
            "'test magnet.gsite'\n",
            "'The Magic of Thinking Big - David J. Schwartz.pdf'\n",
            "'The Secret (Rhonda Byrne)PDF.pdf'\n",
            " Think_and_Grow_Rich.pdf\n",
            "'Trading Alerts 2024'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled project.gscript'\n",
            "'Untitled spreadsheet (1).gsheet'\n",
            "'Untitled spreadsheet.gform'\n",
            "'Untitled spreadsheet.gsheet'\n",
            " updae.gsheet\n",
            "'Videos from Phones'\n",
            "'Videos from Suruchi Phones'\n",
            "'Week1 (Recovered).gslides'\n",
            "'week2 (Recovered).gslides'\n",
            "'week3 (Recovered).gslides'\n",
            "'week4  (Recovered).gslides'\n",
            "'week5 (Recovered).gslides'\n",
            "'week6 (Recovered).gslides'\n",
            "'week7 (Recovered).gslides'\n",
            "'Why testing in production is going to be the next big thing.pdf'\n",
            " wlmod.pdf\n",
            " workload_final.pdf\n",
            "'Workload Modeling and Profiles for Load Testing.pdf'\n",
            "'wow demos-1.pdf'\n",
            "'zl-jitc dod schema design.gsheet'\n",
            " ã¡¡á„á…°á†ºá„„á…°á†¿á„á…°á†¸â¬\n",
            "================================================================================\n",
            "HOSPITAL READMISSION PREDICTION - INFERENCE & ANALYSIS\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Hospital Readmission Prediction - Inference & Analysis\n",
        "=======================================================\n",
        "Features:\n",
        "1. Extract feature importance from all models\n",
        "2. Predict for single patient\n",
        "3. Predict for batch of patients\n",
        "4. Complete metrics table for all models\n",
        "5. Risk stratification and recommendations\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Check that the mount worked\n",
        "!ls /content/drive/MyDrive\n",
        "\n",
        "# TensorFlow for neural networks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, accuracy_score, precision_score, recall_score,\n",
        "    f1_score, brier_score_loss, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HOSPITAL READMISSION PREDICTION - INFERENCE & ANALYSIS\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 1. LOAD ALL MODELS & CONFIGURATION\n",
        "# ============================================================================\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"\\n[1/5] Loading models and configuration...\")\n",
        "# Define base paths\n",
        "BASE_DIR = Path('/content/drive/MyDrive/BerkeleyML/CapstoneHospitalReadmission') # Current directory\n",
        "MODELS_DIR = BASE_DIR / 'modelsf'\n",
        "REPORTS_DIR = BASE_DIR / 'reportsf'\n",
        "DATA_DIR = BASE_DIR / 'eda' # Where processed_data.csv is located\n",
        "\n",
        "# Create directories if they don't exist\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "REPORTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Define specific file paths\n",
        "PATHS = {\n",
        "    # Configuration files\n",
        "    'tree_config': MODELS_DIR / 'tree_models_config.json',\n",
        "    'ensemble_config': MODELS_DIR / 'final_ensemble_config.json',\n",
        "\n",
        "    # Scaler\n",
        "    'scaler': MODELS_DIR / 'scaler_final.pkl',\n",
        "\n",
        "    # Tree-based models\n",
        "    'xgboost': MODELS_DIR / 'xgboost_calibrated_final.pkl',\n",
        "    'lightgbm': MODELS_DIR / 'lightgbm_calibrated_final.pkl',\n",
        "    'random_forest': MODELS_DIR / 'random_forest_calibrated_final.pkl',\n",
        "    'gradient_boosting': MODELS_DIR / 'gradient_boosting_calibrated_final.pkl',\n",
        "    'stacking': MODELS_DIR / 'stacking_ensemble_final.pkl',\n",
        "    'voting': MODELS_DIR / 'voting_ensemble_final.pkl',\n",
        "\n",
        "    # Neural network models\n",
        "    'deep_nn': MODELS_DIR / 'deep_nn_512_256_128_64.h5',\n",
        "    'residual_nn': MODELS_DIR / 'residual_nn.h5',\n",
        "    'attention_nn': MODELS_DIR / 'attention_nn.h5',\n",
        "\n",
        "    # Predictions\n",
        "    'tree_predictions': MODELS_DIR / 'tree_model_predictions.npz',\n",
        "    'nn_predictions': MODELS_DIR / 'final_predictions.npz',\n",
        "    'test_indices': MODELS_DIR / 'test_indices.npy',\n",
        "\n",
        "    # Data\n",
        "    'processed_data': DATA_DIR / 'processed_data.csv',\n",
        "\n",
        "    # Output files\n",
        "    'feature_importance': REPORTS_DIR / 'feature_importance_aggregated.csv',\n",
        "    'metrics_table': REPORTS_DIR / 'all_models_metrics_complete.csv',\n",
        "    'batch_predictions': REPORTS_DIR / 'example_batch_predictions.csv',\n",
        "    'feature_plot': 'top_features_importance.png'\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDAWTvGiRxVt",
        "outputId": "1c0a7bde-39be-4adc-fa2c-7ae7482e1bb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/5] Loading models and configuration...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load configuration\n",
        "with open(PATHS['tree_config'], 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "with open(PATHS['ensemble_config'], 'r') as f:\n",
        "    ensemble_config = json.load(f)\n",
        "\n",
        "# Load scaler\n",
        "scaler = joblib.load(PATHS['scaler'])\n",
        "\n",
        "# Load tree-based models\n",
        "models = {\n",
        "    'XGBoost': joblib.load(PATHS['xgboost']),\n",
        "    'LightGBM': joblib.load(PATHS['lightgbm']),\n",
        "    'Random Forest': joblib.load(PATHS['random_forest']),\n",
        "    'Gradient Boosting': joblib.load(PATHS['gradient_boosting']),\n",
        "    'Stacking Ensemble': joblib.load(PATHS['stacking']),\n",
        "    'Voting Ensemble': joblib.load(PATHS['voting'])\n",
        "}\n",
        "\n",
        "# Load neural networks\n",
        "nn_models = {\n",
        "    'Deep NN': keras.models.load_model(PATHS['deep_nn']),\n",
        "    'Residual NN': keras.models.load_model(PATHS['residual_nn']),\n",
        "    'Attention NN': keras.models.load_model(PATHS['attention_nn'])\n",
        "}\n",
        "\n",
        "# Correcting the model name in ensemble_config\n",
        "ensemble_config['model_names'] = [\n",
        "    'XGBoost', 'LightGBM', 'Random Forest', 'Gradient Boosting',\n",
        "    'Stacking Ensemble', 'Voting Ensemble', 'Deep NN', 'Residual NN',\n",
        "    'Attention NN'\n",
        "]\n",
        "\n",
        "# Load feature names\n",
        "df = pd.read_csv(PATHS['processed_data'])\n",
        "feature_names = [col for col in df.columns if col != 'readmitted_binary']\n",
        "\n",
        "print(f\"âœ“ Loaded {len(models)} tree models + {len(nn_models)} neural networks\")\n",
        "print(f\"âœ“ Features: {len(feature_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX8bvc4eSjoH",
        "outputId": "259489e4-c924-4e22-cf77-fa0f037f2165"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Loaded 6 tree models + 3 neural networks\n",
            "âœ“ Features: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. EXTRACT FEATURE IMPORTANCE FROM ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/5] Extracting feature importance...\")\n",
        "\n",
        "feature_importance_dict = {}\n",
        "\n",
        "# XGBoost feature importance\n",
        "xgb_model = models['XGBoost'].calibrated_classifiers_[0].estimator\n",
        "feature_importance_dict['XGBoost'] = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# LightGBM feature importance\n",
        "lgbm_model = models['LightGBM'].calibrated_classifiers_[0].estimator\n",
        "feature_importance_dict['LightGBM'] = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': lgbm_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Random Forest feature importance\n",
        "rf_model = models['Random Forest'].calibrated_classifiers_[0].estimator\n",
        "feature_importance_dict['Random Forest'] = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Gradient Boosting feature importance\n",
        "gb_model = models['Gradient Boosting'].calibrated_classifiers_[0].estimator\n",
        "feature_importance_dict['Gradient Boosting'] = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': gb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Aggregate feature importance (average across all tree models)\n",
        "all_importances = []\n",
        "for model_name, df_imp in feature_importance_dict.items():\n",
        "    df_imp_copy = df_imp.copy()\n",
        "    df_imp_copy['model'] = model_name\n",
        "    all_importances.append(df_imp_copy)\n",
        "\n",
        "combined_importance = pd.concat(all_importances)\n",
        "aggregate_importance = combined_importance.groupby('feature')['importance'].mean().reset_index()\n",
        "aggregate_importance = aggregate_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nðŸ“Š TOP 20 MOST IMPORTANT FEATURES (Averaged across all tree models):\")\n",
        "print(\"=\"*80)\n",
        "print(aggregate_importance.head(20).to_string(index=False))\n",
        "\n",
        "# Visualize top 15 features\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = aggregate_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['importance'], color='#3498db')\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Average Importance Score', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('Top 15 Most Important Features (Average across Tree Models)',\n",
        "         fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_features_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nâœ“ Feature importance visualization saved: top_features_importance.png\")\n",
        "\n",
        "# Save feature importance to CSV\n",
        "aggregate_importance.to_csv(PATHS['feature_importance'], index=False)\n",
        "print(\"âœ“ Feature importance saved: reports/feature_importance_aggregated.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3YHGK4MTuYW",
        "outputId": "baf24428-f0b9-44af-954c-1fa2d6b1c6a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/5] Extracting feature importance...\n",
            "\n",
            "ðŸ“Š TOP 20 MOST IMPORTANT FEATURES (Averaged across all tree models):\n",
            "================================================================================\n",
            "                 feature  importance\n",
            "         num_medications 1741.774503\n",
            "        time_in_hospital 1133.813516\n",
            "                     age  894.790757\n",
            "        number_diagnoses  855.566561\n",
            "          num_procedures  784.553872\n",
            "discharge_disposition_id  548.382648\n",
            "       admission_type_id  524.780493\n",
            "      num_lab_procedures  455.517226\n",
            "        number_inpatient  425.555332\n",
            "       total_utilization  412.279333\n",
            "       number_outpatient  276.270506\n",
            "     admission_source_id  275.013748\n",
            "                    race  259.262392\n",
            "            labs_per_day  207.513602\n",
            "      procedures_per_day  196.518292\n",
            "               A1Cresult  180.260157\n",
            "        number_emergency  174.018881\n",
            "             age_numeric  173.303570\n",
            "                  gender  160.258674\n",
            "          med_complexity   67.505471\n",
            "\n",
            "âœ“ Feature importance visualization saved: top_features_importance.png\n",
            "âœ“ Feature importance saved: reports/feature_importance_aggregated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3. COMPLETE METRICS TABLE FOR ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3/5] Generating comprehensive metrics table...\")\n",
        "\n",
        "# Load test data for evaluation\n",
        "test_indices = np.load(PATHS['test_indices'])\n",
        "X_test = df.loc[test_indices, feature_names]\n",
        "y_test = df.loc[test_indices, 'readmitted_binary']\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Load all predictions\n",
        "tree_preds = np.load(PATHS['tree_predictions'])\n",
        "nn_preds = np.load(PATHS['nn_predictions'])\n",
        "\n",
        "# Create comprehensive metrics table\n",
        "metrics_list = []\n",
        "\n",
        "def calculate_all_metrics(name, y_true, y_pred, y_proba):\n",
        "    \"\"\"Calculate comprehensive metrics for a model\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'AUC-ROC': roc_auc_score(y_true, y_proba),\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'F1-Score': f1_score(y_true, y_pred, zero_division=0),\n",
        "        'Specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
        "        'NPV': tn / (tn + fn) if (tn + fn) > 0 else 0,\n",
        "        'Brier Score': brier_score_loss(y_true, y_proba),\n",
        "        'True Positives': int(tp),\n",
        "        'True Negatives': int(tn),\n",
        "        'False Positives': int(fp),\n",
        "        'False Negatives': int(fn)\n",
        "    }\n",
        "\n",
        "# Helper function to find optimal threshold\n",
        "def find_optimal_threshold(y_true, y_proba):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    COST_FP = 500\n",
        "    COST_FN = 17500\n",
        "    thresholds = np.linspace(0.1, 0.9, 100)\n",
        "    costs = []\n",
        "    for t in thresholds:\n",
        "        y_pred = (y_proba >= t).astype(int)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        cost = fp * COST_FP + fn * COST_FN\n",
        "        costs.append(cost)\n",
        "    return thresholds[np.argmin(costs)]\n",
        "\n",
        "# Evaluate tree models\n",
        "for model_name in ['xgboost', 'lightgbm', 'random_forest', 'gradient_boosting', 'stacking', 'voting']:\n",
        "    display_name = model_name.replace('_', ' ').title()\n",
        "    y_proba = tree_preds[model_name]\n",
        "    optimal_thresh = find_optimal_threshold(y_test, y_proba)\n",
        "    y_pred = (y_proba >= optimal_thresh).astype(int)\n",
        "    metrics_list.append(calculate_all_metrics(display_name, y_test, y_pred, y_proba))\n",
        "\n",
        "# Evaluate neural networks\n",
        "for model_name in ['deep_nn', 'residual_nn', 'attention_nn']:\n",
        "    display_name = model_name.replace('_', ' ').title()\n",
        "    y_proba = nn_preds[model_name]\n",
        "    optimal_thresh = find_optimal_threshold(y_test, y_proba)\n",
        "    y_pred = (y_proba >= optimal_thresh).astype(int)\n",
        "    metrics_list.append(calculate_all_metrics(display_name, y_test, y_pred, y_proba))\n",
        "\n",
        "# Evaluate final ensemble\n",
        "y_proba_ensemble = nn_preds['final_ensemble']\n",
        "optimal_thresh_ensemble = find_optimal_threshold(y_test, y_proba_ensemble)\n",
        "y_pred_ensemble = (y_proba_ensemble >= optimal_thresh_ensemble).astype(int)\n",
        "metrics_list.append(calculate_all_metrics('Final Ensemble (Softmax)', y_test, y_pred_ensemble, y_proba_ensemble))\n",
        "\n",
        "# Create DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_list)\n",
        "metrics_df = metrics_df.sort_values('AUC-ROC', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE METRICS TABLE - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# Save to CSV\n",
        "metrics_df.to_csv(PATHS['metrics_table'], index=False)\n",
        "print(\"\\nâœ“ Metrics table saved: reports/all_models_metrics_complete.csv\")\n",
        "\n",
        "# Create a prettier display version\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SIMPLIFIED METRICS TABLE (Main Metrics)\")\n",
        "print(\"=\"*80)\n",
        "display_cols = ['Model', 'AUC-ROC', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Brier Score']\n",
        "print(metrics_df[display_cols].to_string(index=False))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0hy1XNKUW_m",
        "outputId": "013853c1-4573-4551-cebc-32b25f0ef69f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3/5] Generating comprehensive metrics table...\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE METRICS TABLE - ALL MODELS\n",
            "================================================================================\n",
            "                   Model  AUC-ROC  Accuracy  Precision   Recall  F1-Score  Specificity      NPV  Brier Score  True Positives  True Negatives  False Positives  False Negatives\n",
            "Final Ensemble (Softmax) 0.662412  0.123514   0.112731 0.997798  0.202575     0.013715 0.980237     0.152241            2266             248            17835                5\n",
            "             Residual Nn 0.657871  0.118208   0.112243 0.999119  0.201814     0.007576 0.985612     0.554691            2269             137            17946                2\n",
            "                 Deep Nn 0.653227  0.122679   0.112674 0.998239  0.202492     0.012719 0.982906     0.550997            2267             230            17853                4\n",
            "            Attention Nn 0.652949  0.124251   0.112815 0.997798  0.202711     0.014544 0.981343     0.553513            2266             263            17820                5\n",
            "                Lightgbm 0.639182  0.669696   0.170905 0.509027  0.255894     0.689874 0.917954     0.097252            1156           12475             5608             1115\n",
            "           Random Forest 0.638226  0.288641   0.124969 0.895641  0.219335     0.212409 0.941883     0.139682            2034            3841            14242              237\n",
            "                  Voting 0.629598  0.711457   0.174322 0.424483  0.247148     0.747498 0.911832     0.096406             964           13517             4566             1307\n",
            "       Gradient Boosting 0.622337  0.641889   0.155452 0.498459  0.236994     0.659902 0.912867     0.096685            1132           11933             6150             1139\n",
            "                 Xgboost 0.586481  0.701680   0.156515 0.381330  0.221937     0.741912 0.905202     0.097338             866           13416             4667             1405\n",
            "                Stacking 0.581545  0.716075   0.162822 0.372963  0.226683     0.759166 0.906019     0.097833             847           13728             4355             1424\n",
            "\n",
            "âœ“ Metrics table saved: reports/all_models_metrics_complete.csv\n",
            "\n",
            "================================================================================\n",
            "SIMPLIFIED METRICS TABLE (Main Metrics)\n",
            "================================================================================\n",
            "                   Model  AUC-ROC  Accuracy  Precision   Recall  F1-Score  Brier Score\n",
            "Final Ensemble (Softmax) 0.662412  0.123514   0.112731 0.997798  0.202575     0.152241\n",
            "             Residual Nn 0.657871  0.118208   0.112243 0.999119  0.201814     0.554691\n",
            "                 Deep Nn 0.653227  0.122679   0.112674 0.998239  0.202492     0.550997\n",
            "            Attention Nn 0.652949  0.124251   0.112815 0.997798  0.202711     0.553513\n",
            "                Lightgbm 0.639182  0.669696   0.170905 0.509027  0.255894     0.097252\n",
            "           Random Forest 0.638226  0.288641   0.124969 0.895641  0.219335     0.139682\n",
            "                  Voting 0.629598  0.711457   0.174322 0.424483  0.247148     0.096406\n",
            "       Gradient Boosting 0.622337  0.641889   0.155452 0.498459  0.236994     0.096685\n",
            "                 Xgboost 0.586481  0.701680   0.156515 0.381330  0.221937     0.097338\n",
            "                Stacking 0.581545  0.716075   0.162822 0.372963  0.226683     0.097833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4. SINGLE PATIENT PREDICTION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4/5] Creating prediction functions...\")\n",
        "\n",
        "def predict_single_patient(patient_data, return_all_models=False):\n",
        "    \"\"\"\n",
        "    Predict readmission risk for a single patient\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    patient_data : dict or pd.DataFrame\n",
        "        Patient features (must match training features)\n",
        "    return_all_models : bool\n",
        "        If True, return predictions from all models\n",
        "        If False, return only final ensemble prediction\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict with prediction results\n",
        "    \"\"\"\n",
        "    # Convert to DataFrame if dict\n",
        "    if isinstance(patient_data, dict):\n",
        "        patient_df = pd.DataFrame([patient_data])\n",
        "    else:\n",
        "        patient_df = patient_data.copy()\n",
        "\n",
        "    # Ensure correct feature order\n",
        "    patient_df = patient_df[feature_names]\n",
        "\n",
        "    # Scale features\n",
        "    patient_scaled = scaler.transform(patient_df)\n",
        "\n",
        "    # Get predictions from all models\n",
        "    predictions = {}\n",
        "\n",
        "    # Tree models\n",
        "    for name, model in models.items():\n",
        "        predictions[name] = model.predict_proba(patient_scaled)[0, 1]\n",
        "\n",
        "    # Neural networks\n",
        "    for name, model in nn_models.items():\n",
        "        predictions[name] = model.predict(patient_scaled, verbose=0)[0, 0]\n",
        "\n",
        "    # Final ensemble (softmax weighted)\n",
        "    softmax_weights = np.array(ensemble_config['softmax_weights'])\n",
        "    model_preds = np.array([predictions[name] for name in ensemble_config['model_names']])\n",
        "    ensemble_prob = np.average(model_preds, weights=softmax_weights)\n",
        "\n",
        "    # Risk stratification\n",
        "    if ensemble_prob >= 0.60:\n",
        "        risk_level = \"HIGH\"\n",
        "        color = \"ðŸ”´\"\n",
        "        action = \"Intensive case management required\"\n",
        "        interventions = [\n",
        "            \"Assign dedicated care coordinator\",\n",
        "            \"Post-discharge phone call within 24 hours\",\n",
        "            \"Home health visit within 48-72 hours\",\n",
        "            \"Pharmacist medication reconciliation\",\n",
        "            \"Schedule 7-day follow-up appointment (not 30-day)\",\n",
        "            \"Daily monitoring for first 2 weeks\"\n",
        "        ]\n",
        "    elif ensemble_prob >= 0.35:\n",
        "        risk_level = \"MEDIUM\"\n",
        "        color = \"ðŸŸ¡\"\n",
        "        action = \"Enhanced discharge planning\"\n",
        "        interventions = [\n",
        "            \"Enhanced discharge instructions (written + verbal)\",\n",
        "            \"Phone call within 7 days\",\n",
        "            \"Schedule 14-day follow-up appointment\",\n",
        "            \"Medication list review\",\n",
        "            \"Patient portal messaging\",\n",
        "            \"Community resource referrals\"\n",
        "        ]\n",
        "    else:\n",
        "        risk_level = \"LOW\"\n",
        "        color = \"ðŸŸ¢\"\n",
        "        action = \"Standard care protocol\"\n",
        "        interventions = [\n",
        "            \"Standard discharge instructions\",\n",
        "            \"30-day routine follow-up\",\n",
        "            \"Patient portal access\",\n",
        "            \"As-needed support\"\n",
        "        ]\n",
        "\n",
        "    result = {\n",
        "        'readmission_probability': ensemble_prob,\n",
        "        'risk_level': risk_level,\n",
        "        'recommended_action': action,\n",
        "        'interventions': interventions,\n",
        "        'cost_avoidance': 17500 if risk_level in ['HIGH', 'MEDIUM'] else 0,\n",
        "        'intervention_cost': 600 if risk_level == 'HIGH' else (200 if risk_level == 'MEDIUM' else 0)\n",
        "    }\n",
        "\n",
        "    if return_all_models:\n",
        "        result['individual_model_predictions'] = predictions\n",
        "\n",
        "    # Print formatted output\n",
        "    print(f\"\\n{color} PREDICTION RESULT {color}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Readmission Probability: {ensemble_prob:.1%}\")\n",
        "    print(f\"Risk Level: {risk_level}\")\n",
        "    print(f\"Recommended Action: {action}\")\n",
        "    print(f\"\\nSuggested Interventions:\")\n",
        "    for i, intervention in enumerate(interventions, 1):\n",
        "        print(f\"  {i}. {intervention}\")\n",
        "    print(f\"\\nFinancial Impact:\")\n",
        "    print(f\"  Intervention Cost: ${result['intervention_cost']:,}\")\n",
        "    print(f\"  Potential Cost Avoided: ${result['cost_avoidance']:,}\")\n",
        "    print(f\"  Net Benefit: ${result['cost_avoidance'] - result['intervention_cost']:,}\")\n",
        "\n",
        "    if return_all_models:\n",
        "        print(f\"\\nIndividual Model Predictions:\")\n",
        "        for model_name, prob in predictions.items():\n",
        "            print(f\"  {model_name:25s} {prob:.3f} ({prob*100:.1f}%)\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def predict_batch_patients(patients_df):\n",
        "    \"\"\"\n",
        "    Predict readmission risk for multiple patients\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    patients_df : pd.DataFrame\n",
        "        DataFrame with patient features (must match training features)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame with predictions and risk levels\n",
        "    \"\"\"\n",
        "    # Ensure correct feature order\n",
        "    patients_df = patients_df[feature_names]\n",
        "\n",
        "    # Scale features\n",
        "    patients_scaled = scaler.transform(patients_df)\n",
        "\n",
        "    # Get ensemble predictions\n",
        "    all_predictions = []\n",
        "\n",
        "    # Collect predictions from all models\n",
        "    for name, model in models.items():\n",
        "        preds = model.predict_proba(patients_scaled)[:, 1]\n",
        "        all_predictions.append(preds)\n",
        "\n",
        "    for name, model in nn_models.items():\n",
        "        preds = model.predict(patients_scaled, verbose=0).flatten()\n",
        "        all_predictions.append(preds)\n",
        "\n",
        "    # Apply softmax weights\n",
        "    all_predictions = np.array(all_predictions).T  # Shape: (n_patients, n_models)\n",
        "    softmax_weights = np.array(ensemble_config['softmax_weights'])\n",
        "    ensemble_probs = np.average(all_predictions, axis=1, weights=softmax_weights)\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = patients_df.copy()\n",
        "    results_df['readmission_probability'] = ensemble_probs\n",
        "\n",
        "    # Risk stratification\n",
        "    results_df['risk_level'] = pd.cut(\n",
        "        ensemble_probs,\n",
        "        bins=[0, 0.35, 0.60, 1.0],\n",
        "        labels=['LOW', 'MEDIUM', 'HIGH']\n",
        "    )\n",
        "\n",
        "    # Intervention costs and potential savings\n",
        "    results_df['intervention_cost'] = results_df['risk_level'].map({\n",
        "        'LOW': 0,\n",
        "        'MEDIUM': 200,\n",
        "        'HIGH': 600\n",
        "    })\n",
        "\n",
        "    results_df['cost_avoidance'] = results_df['risk_level'].map({\n",
        "        'LOW': 0,\n",
        "        'MEDIUM': 17500,\n",
        "        'HIGH': 17500\n",
        "    })\n",
        "\n",
        "    results_df['net_benefit'] = results_df['cost_avoidance'] - results_df['intervention_cost']\n",
        "\n",
        "    print(f\"\\nâœ“ Predicted for {len(results_df)} patients\")\n",
        "    print(f\"\\nRisk Distribution:\")\n",
        "    print(results_df['risk_level'].value_counts().sort_index())\n",
        "    print(f\"\\nTotal Intervention Cost: ${results_df['intervention_cost'].sum():,.0f}\")\n",
        "    print(f\"Total Potential Savings: ${results_df['cost_avoidance'].sum():,.0f}\")\n",
        "    print(f\"Total Net Benefit: ${results_df['net_benefit'].sum():,.0f}\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "print(\"âœ“ Prediction functions created: predict_single_patient(), predict_batch_patients()\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2DqPK0GUpZX",
        "outputId": "64c9e7fd-d1f0-4b13-a823-547c840eea9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4/5] Creating prediction functions...\n",
            "âœ“ Prediction functions created: predict_single_patient(), predict_batch_patients()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5. EXAMPLE USAGE & DEMONSTRATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[5/5] Example usage demonstrations...\")\n",
        "\n",
        "# Example 1: Single patient prediction\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXAMPLE 1: SINGLE PATIENT PREDICTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create example patient (using median values for demo)\n",
        "example_patient = {}\n",
        "for col in feature_names:\n",
        "    example_patient[col] = df[col].median()\n",
        "\n",
        "# Modify some values to create high-risk profile\n",
        "if 'number_inpatient' in feature_names:\n",
        "    example_patient['number_inpatient'] = 2  # High risk factor\n",
        "if 'time_in_hospital' in feature_names:\n",
        "    example_patient['time_in_hospital'] = 8  # Longer stay\n",
        "if 'num_medications' in feature_names:\n",
        "    example_patient['num_medications'] = 20  # Many medications\n",
        "\n",
        "result = predict_single_patient(example_patient, return_all_models=True)\n",
        "\n",
        "# Example 2: Batch prediction (using test set sample)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXAMPLE 2: BATCH PREDICTION (10 PATIENTS)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "sample_patients = df.loc[test_indices[:10], feature_names]\n",
        "batch_results = predict_batch_patients(sample_patients)\n",
        "\n",
        "print(\"\\nBatch Results Preview:\")\n",
        "print(batch_results[['readmission_probability', 'risk_level', 'intervention_cost', 'net_benefit']].head(10).to_string())\n",
        "\n",
        "# Save batch results\n",
        "batch_results.to_csv(PATHS['batch_predictions'], index=False)\n",
        "print(\"\\nâœ“ Batch predictions saved: reports/example_batch_predictions.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MilP_2rXUjKk",
        "outputId": "b74097d7-57f8-4188-b55a-efeb5f1bc384"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[5/5] Example usage demonstrations...\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 1: SINGLE PATIENT PREDICTION\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d7464bef740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d74663a77e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŸ¡ PREDICTION RESULT ðŸŸ¡\n",
            "============================================================\n",
            "Readmission Probability: 39.0%\n",
            "Risk Level: MEDIUM\n",
            "Recommended Action: Enhanced discharge planning\n",
            "\n",
            "Suggested Interventions:\n",
            "  1. Enhanced discharge instructions (written + verbal)\n",
            "  2. Phone call within 7 days\n",
            "  3. Schedule 14-day follow-up appointment\n",
            "  4. Medication list review\n",
            "  5. Patient portal messaging\n",
            "  6. Community resource referrals\n",
            "\n",
            "Financial Impact:\n",
            "  Intervention Cost: $200\n",
            "  Potential Cost Avoided: $17,500\n",
            "  Net Benefit: $17,300\n",
            "\n",
            "Individual Model Predictions:\n",
            "  XGBoost                   0.101 (10.1%)\n",
            "  LightGBM                  0.137 (13.7%)\n",
            "  Random Forest             0.335 (33.5%)\n",
            "  Gradient Boosting         0.120 (12.0%)\n",
            "  Stacking Ensemble         0.099 (9.9%)\n",
            "  Voting Ensemble           0.121 (12.1%)\n",
            "  Deep NN                   0.875 (87.5%)\n",
            "  Residual NN               0.861 (86.1%)\n",
            "  Attention NN              0.824 (82.4%)\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 2: BATCH PREDICTION (10 PATIENTS)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object with dtype category cannot perform the numpy op subtract",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1241271889.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0msample_patients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_batch_patients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_patients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBatch Results Preview:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-132489403.py\u001b[0m in \u001b[0;36mpredict_batch_patients\u001b[0;34m(patients_df)\u001b[0m\n\u001b[1;32m    178\u001b[0m     })\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net_benefit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cost_avoidance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intervention_cost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nâœ“ Predicted for {len(results_df)} patients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__sub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rsub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_align_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_align_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_asobject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# because numexpr will fail on it, see GH#31457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# TODO we should handle EAs consistently and move this check before the if/else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0;31m# for all other cases, raise for now (similarly as what happens in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;31m# Series.__array_prepare__)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;34mf\"Object with dtype {self.dtype} cannot perform \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[0;34mf\"the numpy op {ufunc.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object with dtype category cannot perform the numpy op subtract"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6. CREATE USAGE TEMPLATE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"USAGE TEMPLATE FOR NEW PATIENTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "template_code = \"\"\"\n",
        "# =============================================================================\n",
        "# HOW TO USE FOR NEW PATIENTS\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load this inference script\n",
        "# exec(open('model_inference.py').read())\n",
        "\n",
        "# ============================================================================\n",
        "# METHOD 1: Predict for a SINGLE patient\n",
        "# ============================================================================\n",
        "\n",
        "# Create patient data dictionary\n",
        "new_patient = {\n",
        "    'age_numeric': 65,\n",
        "    'time_in_hospital': 5,\n",
        "    'num_lab_procedures': 45,\n",
        "    'num_procedures': 3,\n",
        "    'num_medications': 15,\n",
        "    'number_outpatient': 2,\n",
        "    'number_emergency': 1,\n",
        "    'number_inpatient': 0,\n",
        "    # ... add all required features\n",
        "}\n",
        "\n",
        "# Get prediction\n",
        "result = predict_single_patient(new_patient, return_all_models=True)\n",
        "\n",
        "# Access results\n",
        "print(f\"Risk: {result['readmission_probability']:.1%}\")\n",
        "print(f\"Level: {result['risk_level']}\")\n",
        "print(f\"Action: {result['recommended_action']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# METHOD 2: Predict for MULTIPLE patients (batch)\n",
        "# ============================================================================\n",
        "\n",
        "# Load patient data from CSV or database\n",
        "patients_df = pd.read_csv('new_patients.csv')\n",
        "\n",
        "# Get predictions\n",
        "predictions_df = predict_batch_patients(patients_df)\n",
        "\n",
        "# Save results\n",
        "predictions_df.to_csv('predictions_output.csv', index=False)\n",
        "\n",
        "# Filter high-risk patients\n",
        "high_risk = predictions_df[predictions_df['risk_level'] == 'HIGH']\n",
        "print(f\"High-risk patients: {len(high_risk)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# METHOD 3: Real-time API integration\n",
        "# ============================================================================\n",
        "\n",
        "def api_predict(patient_json):\n",
        "    '''\n",
        "    Wrapper for API integration\n",
        "    '''\n",
        "    import json\n",
        "    patient_dict = json.loads(patient_json)\n",
        "    result = predict_single_patient(patient_dict)\n",
        "    return json.dumps(result, default=str)\n",
        "\n",
        "# Example API call\n",
        "patient_json = '{\"age_numeric\": 65, \"time_in_hospital\": 5, ...}'\n",
        "response = api_predict(patient_json)\n",
        "\"\"\"\n",
        "\n",
        "print(template_code)\n",
        "\n",
        "# Save template\n",
        "with open('prediction_usage_template.py', 'w') as f:\n",
        "    f.write(template_code)\n",
        "\n",
        "print(\"\\nâœ“ Usage template saved: prediction_usage_template.py\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpkrTkGFUgGm",
        "outputId": "8d08dbe2-30e9-4c61-c994-d4e19ebd1a38"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "USAGE TEMPLATE FOR NEW PATIENTS\n",
            "================================================================================\n",
            "\n",
            "# =============================================================================\n",
            "# HOW TO USE FOR NEW PATIENTS\n",
            "# =============================================================================\n",
            "\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "\n",
            "# Load this inference script\n",
            "# exec(open('model_inference.py').read())\n",
            "\n",
            "# ============================================================================\n",
            "# METHOD 1: Predict for a SINGLE patient\n",
            "# ============================================================================\n",
            "\n",
            "# Create patient data dictionary\n",
            "new_patient = {\n",
            "    'age_numeric': 65,\n",
            "    'time_in_hospital': 5,\n",
            "    'num_lab_procedures': 45,\n",
            "    'num_procedures': 3,\n",
            "    'num_medications': 15,\n",
            "    'number_outpatient': 2,\n",
            "    'number_emergency': 1,\n",
            "    'number_inpatient': 0,\n",
            "    # ... add all required features\n",
            "}\n",
            "\n",
            "# Get prediction\n",
            "result = predict_single_patient(new_patient, return_all_models=True)\n",
            "\n",
            "# Access results\n",
            "print(f\"Risk: {result['readmission_probability']:.1%}\")\n",
            "print(f\"Level: {result['risk_level']}\")\n",
            "print(f\"Action: {result['recommended_action']}\")\n",
            "\n",
            "# ============================================================================\n",
            "# METHOD 2: Predict for MULTIPLE patients (batch)\n",
            "# ============================================================================\n",
            "\n",
            "# Load patient data from CSV or database\n",
            "patients_df = pd.read_csv('new_patients.csv')\n",
            "\n",
            "# Get predictions\n",
            "predictions_df = predict_batch_patients(patients_df)\n",
            "\n",
            "# Save results\n",
            "predictions_df.to_csv('predictions_output.csv', index=False)\n",
            "\n",
            "# Filter high-risk patients\n",
            "high_risk = predictions_df[predictions_df['risk_level'] == 'HIGH']\n",
            "print(f\"High-risk patients: {len(high_risk)}\")\n",
            "\n",
            "# ============================================================================\n",
            "# METHOD 3: Real-time API integration\n",
            "# ============================================================================\n",
            "\n",
            "def api_predict(patient_json):\n",
            "    '''\n",
            "    Wrapper for API integration\n",
            "    '''\n",
            "    import json\n",
            "    patient_dict = json.loads(patient_json)\n",
            "    result = predict_single_patient(patient_dict)\n",
            "    return json.dumps(result, default=str)\n",
            "\n",
            "# Example API call\n",
            "patient_json = '{\"age_numeric\": 65, \"time_in_hospital\": 5, ...}'\n",
            "response = api_predict(patient_json)\n",
            "\n",
            "\n",
            "âœ“ Usage template saved: prediction_usage_template.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… COMPLETE! ALL ARTIFACTS GENERATED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nGenerated Files:\")\n",
        "print(\"  ðŸ“Š reports/feature_importance_aggregated.csv\")\n",
        "print(\"  ðŸ“Š reports/all_models_metrics_complete.csv\")\n",
        "print(\"  ðŸ“Š reports/example_batch_predictions.csv\")\n",
        "print(\"  ðŸ“ˆ top_features_importance.png\")\n",
        "print(\"  ðŸ“ prediction_usage_template.py\")\n",
        "\n",
        "print(\"\\nKey Functions Available:\")\n",
        "print(\"  â€¢ predict_single_patient(patient_data, return_all_models=False)\")\n",
        "print(\"  â€¢ predict_batch_patients(patients_df)\")\n",
        "\n",
        "print(\"\\nTop 5 Most Important Features:\")\n",
        "for i, row in aggregate_importance.head(5).iterrows():\n",
        "    print(f\"  {i+1}. {row['feature']:30s} (importance: {row['importance']:.4f})\")\n",
        "\n",
        "best_model = metrics_df.iloc[0]\n",
        "print(f\"\\nBest Performing Model: {best_model['Model']}\")\n",
        "print(f\"  AUC-ROC: {best_model['AUC-ROC']:.4f}\")\n",
        "print(f\"  Precision: {best_model['Precision']:.4f}\")\n",
        "print(f\"  Recall: {best_model['Recall']:.4f}\")\n",
        "print(f\"  F1-Score: {best_model['F1-Score']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Ready for production deployment! ðŸš€\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5deOr--UZ0S",
        "outputId": "d43c8c33-8c6a-4ac6-f265-68fa62295cc0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "âœ… COMPLETE! ALL ARTIFACTS GENERATED\n",
            "================================================================================\n",
            "\n",
            "Generated Files:\n",
            "  ðŸ“Š reports/feature_importance_aggregated.csv\n",
            "  ðŸ“Š reports/all_models_metrics_complete.csv\n",
            "  ðŸ“Š reports/example_batch_predictions.csv\n",
            "  ðŸ“ˆ top_features_importance.png\n",
            "  ðŸ“ prediction_usage_template.py\n",
            "\n",
            "Key Functions Available:\n",
            "  â€¢ predict_single_patient(patient_data, return_all_models=False)\n",
            "  â€¢ predict_batch_patients(patients_df)\n",
            "\n",
            "Top 5 Most Important Features:\n",
            "  34. num_medications                (importance: 1741.7745)\n",
            "  45. time_in_hospital               (importance: 1133.8135)\n",
            "  6. age                            (importance: 894.7908)\n",
            "  36. number_diagnoses               (importance: 855.5666)\n",
            "  35. num_procedures                 (importance: 784.5539)\n",
            "\n",
            "Best Performing Model: Final Ensemble (Softmax)\n",
            "  AUC-ROC: 0.6624\n",
            "  Precision: 0.1127\n",
            "  Recall: 0.9978\n",
            "  F1-Score: 0.2026\n",
            "\n",
            "================================================================================\n",
            "Ready for production deployment! ðŸš€\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}